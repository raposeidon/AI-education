{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1_OiU-0ow4_C5HYwvLrIU_V6Dyv93ENGd",
      "authorship_tag": "ABX9TyMP5UFsyStUClIVu8As6DN7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7RVimegf3OS"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q trl xformers wandb datasets einops sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings, torch, wandb, platform, transformers, peft, datasets, trl\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "cu4OStWGk5bt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Torch version : {torch.__version__}')\n",
        "print(f'Wandb version : {wandb.__version__}')\n",
        "print(f'Transformers version : {transformers.__version__}')\n",
        "print(f'Peft version : {peft.__version__}')\n",
        "print(f'Datasets version : {datasets.__version__}')\n",
        "print(f'Trl version : {trl.__version__}')"
      ],
      "metadata": {
        "id": "f0AfbGh5k5d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load auth\n",
        "import json\n",
        "filename ='./drive/MyDrive/Colab Notebooks/config.json'\n",
        "with open(filename, \"r\") as file:\n",
        "            api_keys = json.load(file)"
      ],
      "metadata": {
        "id": "95VF1zqWk5f_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model, dataset_name, new_model = 'mistralai/Mistral-7B-v0.1', 'gathnex/Gath_baize', 'gathnex/Gath_mistral_7b'\n"
      ],
      "metadata": {
        "id": "IagmaMFlk5iR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split='train')\n",
        "print(dataset['chat_sample'][0])\n",
        "print(dataset.shape)\n",
        "dataset = dataset.shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "CnegtCPWk5kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 베이스 모델 불러오기\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\":0}\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# 토크나이저 불러오기\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.add_bos_token, tokenizer.add_eos_token"
      ],
      "metadata": {
        "id": "G3MopBItnJuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
        "    )\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "FcMxRJ3Dn16l"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_keys.get(\"api_keys\")"
      ],
      "metadata": {
        "id": "oTNbVS2Kof9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM 모니터링\n",
        "wandb.login(key = api_keys.get(\"api_keys\"))\n",
        "run = wandb.init(project='Fine tuning mistral 7B', job_type='training', anonymous='allow')\n"
      ],
      "metadata": {
        "id": "Jn-rAWRfoNiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim='paged_adamw_8bit',\n",
        "    save_steps=5000,\n",
        "    logging_steps=30,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.3,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type='constant',\n",
        "    report_to='wandb'\n",
        ")\n",
        "\n",
        "# sft 파라미터\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=None,\n",
        "    dataset_text_field='chat_sample',\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False\n",
        ")"
      ],
      "metadata": {
        "id": "af5IlfkxpA_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "# save fine tuning model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "wandb.finish()\n",
        "model.config.use_cache = True\n",
        "model.eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "OGEddmn6pJld",
        "outputId": "b9bed01e-bb46-4551-daed-6cc2585dfad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 11/500 03:15 < 2:57:24, 0.05 it/s, Epoch 0.04/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='127' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [127/500 35:40 < 1:46:26, 0.06 it/s, Epoch 0.50/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.824700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.915200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.757400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.881600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}